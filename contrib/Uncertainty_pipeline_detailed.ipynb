{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scripts.prediction_utils as predict_u\n",
    "import scripts.theodolite_utils as theodo_u\n",
    "import scripts.theodolite_plot_function as theodo_p\n",
    "import scripts.groundtruth_utils as theodo_g\n",
    "import scripts.theodolite_function as theodo_f\n",
    "import time\n",
    "import torch\n",
    "from numpy import linalg\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEDS\n",
    "random.seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(progress_values:list, sep:str=\"\\n\"):\n",
    "    ratios = [f\"{p['idx']}/{p['total']}\" for p in progress_values]\n",
    "    with open(\"progress.log\", \"a\", encoding=\"utf-8\") as f:\n",
    "        print(\" : \".join(ratios), file=f, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################ Partie Ã  modifier ######################################################################\n",
    "np.random.seed(10)\n",
    "## Path of main data files\n",
    "path = [\n",
    "    \"../data/20220224/\",        # 1 done Instrument, time synch\n",
    "    \"../data/20220307/\",        # 2 done Instrument, time synch\n",
    "    \"../data/20220312/\",        # 3 done Instrument, time synch\n",
    "    \"../data/20220314/\",        # 4 done Instrument, time synch\n",
    "    \"../data/20220316/\",        # 5 done Instrument, time synch\n",
    "    \"../data/20220331-1/\",      # 6 done Instrument, time synch\n",
    "    \"../data/20220331-2/\",      # 7 done Instrument, time synch\n",
    "    \"../data/20220513-1/\",      # 10 done Instrument, time synch\n",
    "    \"../data/20220513-2/\",      # 11 done Instrument, time synch\n",
    "    \"../data/20220513-3/\",      # 12 done Instrument, time synch\n",
    "    \"../data/20220513-4/\",      # 13 done Instrument, time synch\n",
    "    \"../data/20220513-5/\",      # 14 done Instrument, time synch\n",
    "    \"../data/20220513-6/\",      # 15 done Instrument, time synch\n",
    "    \"../data/20220525-1/\",      # 16 done Instrument, time synch\n",
    "    \"../data/20220525-2/\",      # 17 done Instrument, time synch\n",
    "    \"../data/20220622-1/\",      # 18 done Instrument, time synch\n",
    "    \"../data/20220622-2/\",      # 19 done Instrument, time synch\n",
    "    \"../data/20220630-1/\",      # 20 done Instrument, time synch\n",
    "    \"../data/20220630-2/\",      # 21 done Instrument, time synch\n",
    "    \"../data/20220711-1/\",      # 22 done Instrument, time synch\n",
    "    \"../data/20220711-2/\",      # 23 done Instrument, time synch\n",
    "    \"../data/20220715-1/\",      # 24 done Instrument, time synch\n",
    "    \"../data/20220715-2/\",      # 25 done Instrument, time synch\n",
    "    \"../data/20220715-3/\",      # 26 done Instrument, time synch\n",
    "    \"../data/20220715-4/\",      # 27 done Instrument, time synch\n",
    "    \"../data/20221103-1/\",      # 28 done Instrument, time synch\n",
    "    \"../data/20221103-2/\",      # 29 done Instrument, time synch\n",
    "    \"../data/20221103-3/\",      # 30 done Instrument, time synch\n",
    "    \"../data/20221116-1/\",      # 35 done Instrument, time synch\n",
    "    \"../data/20221123/\",        # 36 done Instrument, time synch\n",
    "    \"../data/20221124/\",        # 37 done Instrument, time synch\n",
    "    \"../data/20221129-1/\",      # 38 done Instrument, time synch\n",
    "    \"../data/20221129-2/\",      # 39 done Instrument, time synch\n",
    "    \"../data/20221129-3/\",      # 40 done Instrument, time synch\n",
    "    \"../data/20221129-4/\",      # 41 done Instrument, time synch\n",
    "    \"../data/20221129-5/\",      # 42 done Instrument, time synch\n",
    "    \"../data/20221205-1/\",      # 43 done Instrument, time synch\n",
    "    \"../data/20221205-2/\",      # 44 done Instrument, time synch\n",
    "    \"../data/20221205-3/\",      # 45 done Instrument, time synch\n",
    "    \"../data/20220427-1/\",      # 8 done Instrument, time synch\n",
    "    \"../data/20220427-2/\",      # 9 done Instrument, time synch\n",
    "    \"../data/20221109-1/\",      # 31 done Instrument, time synch\n",
    "    \"../data/20221109-2/\",      # 32 done Instrument, time synch\n",
    "    \"../data/20221109-3/\",      # 33 done Instrument, time synch\n",
    "    \"../data/20221110/\",        # 34 done Instrument, atmospheric\n",
    "]\n",
    "\n",
    "weather_list = [\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 1,\n",
    "    1, 1, 1, 1, 1,\n",
    "]\n",
    "\n",
    "## Choice of noise model\n",
    "# Tilt noise, Atmospheric correction, extrinsic calibration, Time synchronization, Weather corrections # 1: Activate, 0:Disable\n",
    "model_chosen = [0,1,0,0,0]\n",
    "\n",
    "#############################################################################################################################################################\n",
    "#############################################################################################################################################################\n",
    "type_file_input = \"CSV\"    ## CSV or ROSBAG\n",
    "\n",
    "## Parameters for pre-processing pipeline\n",
    "parameters = [\n",
    "    [1,2,1,1,3,2],  # 1. Apply filtering or not (Module 1), 2-3-4. Parameters tau_r, tau_a, tau_e (Module 1), 5. Parameter tau_s (Module 2), 6 Parameter tau_l (Module 4).\n",
    "]\n",
    "\n",
    "# Reading sensor data\n",
    "# Sensor = \"Robosense_32\"\n",
    "Sensor = \"Fake\"   # GNSS or Robosense_32 or Fake\n",
    "# path_sensor_file = path+\"ICP/icp_odom.csv\"\n",
    "rate_fake = 5  # Hz\n",
    "Sensor_fake = \"GNSS\"\n",
    "path_sensor_file = \"gps_data/\"+\"gps1.txt\"\n",
    "path_sensor_file_synch_time = \"gps_data/delay_synchronization_GNSS_1.txt\"  # If applied to GNSS\n",
    "Gps_reference_chosen = 1    # 1: front, 2: back, 3: middle   # Only for GNSS\n",
    "\n",
    "# Limit acceptable for ground truth error\n",
    "limit_taken_into_accout = False\n",
    "limit_dist = 100  # Need to check the impact\n",
    "limit_uncertainty = 100 # Limit of eigenvalues taken into account\n",
    "\n",
    "## Parameters about noise for MC\n",
    "## Number sample MC\n",
    "num_samples = 1000\n",
    "## Range noise\n",
    "random_noise_range = [0, 0.004/2, 2]   ## Mean, sigma, ppm,  4mm + 2ppm (2 sigma)  ISO17123-3\n",
    "## Angles noise\n",
    "random_noise_angle = [0, 0.000024241/5*4/2]    # Mean, sigma, 5\"=0.000024241 precision datasheet  (2 sigma)  ISO17123-3\n",
    "## Tilt compensator noise\n",
    "random_noise_tilt = [0, 0.000002424/2]    # Mean, sigma, 0.5\"=0.000002424 precision datasheet  (2 sigma)  ISO17123-3\n",
    "## Weather conditions\n",
    "weather_data_path = \"../data/weather_2022/\"\n",
    "data_weather_quebec = np.array(theodo_u.read_weather_data(weather_data_path+\"Quebec/data_sorted_2022.txt\"))\n",
    "data_weather_fm = np.array(theodo_u.read_weather_data(weather_data_path+\"Montmorency_forest/data_sorted_2022.txt\"))\n",
    "## Time synchronization\n",
    "time_error_synch_mean = 1.157*10**(-3)  # Mean time error [s]\n",
    "time_error_synch_std = 0.815*10**(-3)   # Std time error [s]\n",
    "\n",
    "num_samples_MC_sensor = 100   ## Number sample for uncertainty propagation to the sensor\n",
    "save_MC_inteprolated = True   ## Save interpolation done with STEAM\n",
    "want_sensor_pose = False      ## Compute sensor uncertainty\n",
    "save_MC_sensor = False        ## Save sensor pose with uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/45 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed:  ../data/20220224/\n",
      "Parameters used:  [1, 2, 1, 1, 3, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damien/code/norlab/theodolite/RTS_project/contrib/../scripts/theodolite_utils.py:1699: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array_point = np.array([Time, D, A, E, Speed, Speed_sigma])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sub-trajectories : 44\n",
      "Sub-trajectories 0\n",
      "Number of input:  85\n",
      "Interpolation MC done !\n",
      "Interpolation MC done !\n",
      "Interpolation MC done !\n",
      "Time [s]:  88.15\n",
      "Sub-trajectories 1\n",
      "Number of input:  15\n",
      "Interpolation MC done !\n",
      "Interpolation MC done !\n",
      "Interpolation MC done !\n",
      "Time [s]:  11.77\n",
      "Sub-trajectories 2\n",
      "Number of input:  19\n",
      "Interpolation MC done !\n",
      "Interpolation MC done !\n",
      "Interpolation MC done !\n",
      "Time [s]:  15.94\n",
      "Sub-trajectories 3\n",
      "Number of input:  54\n"
     ]
    }
   ],
   "source": [
    "n_paths = len(path)\n",
    "idx = 0\n",
    "for file_i, weather_index in zip(tqdm(path),weather_list):\n",
    "    print(\"File processed: \", file_i)\n",
    "    # Reading sensor extrinsic calibration\n",
    "    file_sensors = theodo_u.if_file_exist(file_i + \"sensors_extrinsic_calibration/calibration_results.csv\",'')\n",
    "    extrinsic_calibration_results = theodo_u.read_extrinsic_calibration_results_file(file_sensors)\n",
    "\n",
    "    ## Read sensor data which we want a ground truth\n",
    "    sensor_data = []\n",
    "    if Sensor == \"GNSS\":\n",
    "        GNSS_raw_data = theodo_u.read_prediction_data_Linear_csv_file(file_i+path_sensor_file)\n",
    "        time_delay = float(theodo_u.read_time_delay(file_i+path_sensor_file_synch_time))\n",
    "\n",
    "        for i in GNSS_raw_data:\n",
    "            raw_data = np.array([i[0] + time_delay, i[1], i[2], i[3], 0, 0, 0, 1])\n",
    "            sensor_data.append(raw_data)\n",
    "        sensor_data = np.array(sensor_data)\n",
    "\n",
    "    if Sensor == \"Robosense_32\":\n",
    "        sensor_data = theodo_u.read_icp_odom_file(file_i+path_sensor_file)\n",
    "        sensor_data = np.array(sensor_data)\n",
    "\n",
    "    ## Number of rigid transforms to apply the uncertainty\n",
    "    path_file_GCP = \"total_stations/GCP.txt\"\n",
    "    frame_chosen, T_1_corrected, T_2_corrected, T_3_corrected = theodo_g.extrinsic_calibration_noise(file_i+path_file_GCP, random_noise_range, random_noise_angle, random_noise_tilt, num_samples)\n",
    "    ## Read Extrinsic results for RTS\n",
    "    ## Estimated extrinsic calibration uncertainty\n",
    "    _, _, _, Tf_1, Tf_2, Tf_3 = theodo_u.read_marker_file(file_i+path_file_GCP, 1, 1)\n",
    "\n",
    "    if weather_index==0:\n",
    "        data_weather=data_weather_quebec\n",
    "    else:\n",
    "        if weather_index==1:\n",
    "            data_weather=data_weather_fm\n",
    "\n",
    "    for param in parameters:\n",
    "        print(\"Parameters used: \", param)\n",
    "\n",
    "        ## Parameters to process the rosbag data\n",
    "        if param[0] == 0:\n",
    "            filtering = False\n",
    "        if param[0] == 1:\n",
    "            filtering = True\n",
    "        thresold_d = param[1]  ## tau_r [m/s]\n",
    "        thresold_a = param[2]  ## tau_a [deg/s]\n",
    "        thresold_e = param[3]  ## tau_e [deg/s]\n",
    "        limit_time_interval = param[4]  ## tau_s [s]\n",
    "        limit_search = limit_time_interval\n",
    "        size_interval = param[5]    ## tau_l\n",
    "        Mode = \"STEAM\"  ## Interpolation choice: 1. L -> Linear interpolation, 2. SGP -> Gaussian Process with Stheno library, 3. STEAM\n",
    "\n",
    "        ## Open rosbag with outlier filters\n",
    "        # if type_file_input==\"ROSBAG\":\n",
    "        #     if filtering:\n",
    "        #         t1, t2, t3, _, _, _, d1, d2, d3, a1, a2, a3, e1, e2, e3 = theodo_u.read_rosbag_theodolite_without_tf_raw_data_pre_filtered(\n",
    "        #             file_d)\n",
    "        #         index_1_f = theodo_u.thresold_raw_data(t1, d1, a1, e1, thresold_d, thresold_a * 3.1415926 / 180,\n",
    "        #                                                thresold_e * 3.1415926 / 180, limit_time_interval)\n",
    "        #         index_2_f = theodo_u.thresold_raw_data(t2, d2, a2, e2, thresold_d, thresold_a * 3.1415926 / 180,\n",
    "        #                                                thresold_e * 3.1415926 / 180, limit_time_interval)\n",
    "        #         index_3_f = theodo_u.thresold_raw_data(t3, d3, a3, e3, thresold_d, thresold_a * 3.1415926 / 180,\n",
    "        #                                                thresold_e * 3.1415926 / 180, limit_time_interval)\n",
    "        #         t1 = t1[index_1_f]\n",
    "        #         t2 = t2[index_2_f]\n",
    "        #         t3 = t3[index_3_f]\n",
    "        #         print(\"Size of prism measurements 1, 2 and 3: \", len(t1), len(t2), len(t3))\n",
    "        #     else:\n",
    "        #         t1, t2, t3, _, _, _, d1, d2, d3, a1, a2, a3, e1, e2, e3 = theodo_u.read_rosbag_theodolite_without_tf_raw_data(\n",
    "        #             file_d)\n",
    "        #         print(\"Size of prism measurements 1, 2 and 3: \", len(t1), len(t2), len(t3))\n",
    "\n",
    "        if type_file_input==\"CSV\":\n",
    "            sub_path = \"uncertainty/raw_data/\"\n",
    "            P1 = theodo_u.read_raw_data_uncertainty_speed(file_i+sub_path+\"speed_prism1.csv\")\n",
    "            P2 = theodo_u.read_raw_data_uncertainty_speed(file_i+sub_path+\"speed_prism2.csv\")\n",
    "            P3 = theodo_u.read_raw_data_uncertainty_speed(file_i+sub_path+\"speed_prism3.csv\")\n",
    "            t1, t2, t3, d1, d2, d3, a1, a2, a3, e1, e2, e3, s1, s2, s3, ss1, ss2, ss3 = ([] for i in range(18))\n",
    "            for p1_i, p2_i, p3_i in zip(P1, P2, P3):\n",
    "                t1.append(p1_i[0])\n",
    "                t2.append(p2_i[0])\n",
    "                t3.append(p3_i[0])\n",
    "                d1.append(p1_i[1])\n",
    "                d2.append(p2_i[1])\n",
    "                d3.append(p3_i[1])\n",
    "                a1.append(p1_i[2])\n",
    "                a2.append(p2_i[2])\n",
    "                a3.append(p3_i[2])\n",
    "                e1.append(p1_i[3])\n",
    "                e2.append(p2_i[3])\n",
    "                e3.append(p3_i[3])\n",
    "                s1.append(p1_i[4])\n",
    "                s2.append(p2_i[4])\n",
    "                s3.append(p3_i[4])\n",
    "                ss1.append(p1_i[5])\n",
    "                ss2.append(p2_i[5])\n",
    "                ss3.append(p3_i[5])\n",
    "            t1 = np.array(t1)\n",
    "            t2 = np.array(t2)\n",
    "            t3 = np.array(t3)\n",
    "            d1 = np.array(d1)\n",
    "            d2 = np.array(d2)\n",
    "            d3 = np.array(d3)\n",
    "            a1 = np.array(a1)\n",
    "            a2 = np.array(a2)\n",
    "            a3 = np.array(a3)\n",
    "            e1 = np.array(e1)\n",
    "            e2 = np.array(e2)\n",
    "            e3 = np.array(e3)\n",
    "            s1 = np.array(s1)\n",
    "            s2 = np.array(s2)\n",
    "            s3 = np.array(s3)\n",
    "            ss1 = np.array(ss1)\n",
    "            ss2 = np.array(ss2)\n",
    "            ss3 = np.array(ss3)\n",
    "\n",
    "        ## Split trajectories according to tau_s\n",
    "        list_interval, list_time = theodo_f.split_time_interval_all_data(t1, t2, t3, limit_time_interval)\n",
    "        list_trajectories_split = theodo_f.merge_interval(list_interval, list_time, t1, t2, t3, limit_search)\n",
    "\n",
    "        M_1_before_inter = []\n",
    "        M_2_before_inter = []\n",
    "        M_3_before_inter = []\n",
    "        Prediction_1 = []\n",
    "        Prediction_2 = []\n",
    "        Prediction_3 = []\n",
    "        T_prediction = []\n",
    "        Index_sensor = []\n",
    "\n",
    "        print(\"Number of sub-trajectories :\", len(list_trajectories_split))\n",
    "        number_ite = 0\n",
    "        ## Process each of the sub-trajectories\n",
    "        traj_num = len(list_trajectories_split)\n",
    "        for traj_idx, i in enumerate(list_trajectories_split):\n",
    "            print(\"Sub-trajectories \" + str(number_ite))\n",
    "            number_ite = number_ite + 1\n",
    "            index_1 = np.array([i[0, 0], i[1, 0]])\n",
    "            index_2 = np.array([i[0, 1], i[1, 1]])\n",
    "            index_3 = np.array([i[0, 2], i[1, 2]])\n",
    "\n",
    "            begin = np.max([t1[index_1[0]], t2[index_2[0]], t3[index_3[0]]])\n",
    "            end = np.min([t1[index_1[1]], t2[index_2[1]], t3[index_3[1]]])\n",
    "\n",
    "            if abs(end - begin) > size_interval and begin < end:  # control number of sample\n",
    "\n",
    "                ## If fake querrying, apply desired rate\n",
    "                T_prediction_sensor = []\n",
    "                if Sensor!=\"Fake\":\n",
    "                    Number = 0\n",
    "                    for value_sensor_data in sensor_data:\n",
    "                        if end >= value_sensor_data[0] >= begin:\n",
    "                            T_prediction_sensor.append(value_sensor_data[0])\n",
    "                            Index_sensor.append(Number)\n",
    "                        Number = Number + 1\n",
    "\n",
    "                    T_prediction_init = torch.from_numpy(np.array(T_prediction_sensor))\n",
    "                else:\n",
    "                    List_time = np.arange(begin, end, 1/rate_fake)\n",
    "                    for i in List_time:\n",
    "                        T_prediction_sensor.append(i)\n",
    "                    T_prediction_init = torch.from_numpy(np.arange(begin, end, 1/rate_fake))\n",
    "\n",
    "                if Mode == \"STEAM\" or Mode == \"All\":\n",
    "\n",
    "                    MC_1 = []\n",
    "                    MC_2 = []\n",
    "                    MC_3 = []\n",
    "                    tic = time.perf_counter()\n",
    "                    ## Compute uncertainty\n",
    "                    T1, D1, A1, E1, S1, SS1 = predict_u.data_training_L_Raw_data(t1, d1, a1, e1, s1, ss1, index_1)\n",
    "                    T2, D2, A2, E2, S2, SS2 = predict_u.data_training_L_Raw_data(t2, d2, a2, e2, s2, ss2, index_2)\n",
    "                    T3, D3, A3, E3, S3, SS3 = predict_u.data_training_L_Raw_data(t3, d3, a3, e3, s3, ss3, index_3)\n",
    "                    print(\"Number of input: \", len(T1))\n",
    "\n",
    "                    for i1,j1,k1,l1,m1,n1 in zip(T1,D1,A1,E1,S1,SS1):\n",
    "                        mu_raw_data, _,cov_matrix_simulated = theodo_g.MC_raw_data(num_samples, j1, random_noise_range, k1, l1, random_noise_angle, random_noise_tilt, Tf_1, T_1_corrected, data_weather, i1, m1, n1, time_error_synch_mean, time_error_synch_std, model_chosen)\n",
    "                        MC_1.append([i1,mu_raw_data,cov_matrix_simulated])\n",
    "                        M_1_before_inter.append([i1,mu_raw_data,cov_matrix_simulated])\n",
    "                    for i2,j2,k2,l2,m2,n2 in zip(T2,D2,A2,E2,S2,SS2):\n",
    "                        mu_raw_data, _,cov_matrix_simulated = theodo_g.MC_raw_data(num_samples, j2, random_noise_range, k2, l2, random_noise_angle, random_noise_tilt, Tf_2, T_2_corrected, data_weather, i2, m2, n2, time_error_synch_mean, time_error_synch_std, model_chosen)\n",
    "                        MC_2.append([i2,mu_raw_data,cov_matrix_simulated])\n",
    "                        M_2_before_inter.append([i2,mu_raw_data,cov_matrix_simulated])\n",
    "                    for i3,j3,k3,l3,m3,n3 in zip(T3,D3,A3,E3,S3,SS3):\n",
    "                        mu_raw_data, _,cov_matrix_simulated = theodo_g.MC_raw_data(num_samples, j3, random_noise_range, k3, l3, random_noise_angle, random_noise_tilt, Tf_3, T_3_corrected, data_weather, i3, m3, n3, time_error_synch_mean, time_error_synch_std, model_chosen)\n",
    "                        MC_3.append([i3,mu_raw_data,cov_matrix_simulated])\n",
    "                        M_3_before_inter.append([i3,mu_raw_data,cov_matrix_simulated])\n",
    "\n",
    "                    ## STEAM\n",
    "                    MC_1_interpolated = theodo_g.STEAM_interpolation_with_covariance(T1, T_prediction_sensor, MC_1)\n",
    "                    MC_2_interpolated = theodo_g.STEAM_interpolation_with_covariance(T2, T_prediction_sensor, MC_2)\n",
    "                    MC_3_interpolated = theodo_g.STEAM_interpolation_with_covariance(T3, T_prediction_sensor, MC_3)\n",
    "                    toc = time.perf_counter()\n",
    "                    print(\"Time [s]: \", round(toc-tic,2))\n",
    "                    if(len(MC_1_interpolated)>0 and len(MC_2_interpolated)>0 and len(MC_3_interpolated)>0):\n",
    "                        Prediction_1.append(MC_1_interpolated)\n",
    "                        Prediction_2.append(MC_2_interpolated)\n",
    "                        Prediction_3.append(MC_3_interpolated)\n",
    "                        T_prediction.append(T_prediction_sensor)\n",
    "        \n",
    "            log_progress([{\"idx\":idx, \"total\":n_paths},{\"idx\": traj_idx, \"total\":traj_num}])\n",
    "\n",
    "        MC_1_inter = []\n",
    "        MC_2_inter = []\n",
    "        MC_3_inter = []\n",
    "        for i1,j1,k1 in zip(Prediction_1,Prediction_2,Prediction_3):\n",
    "            for i2,j2,k2 in zip(i1,j1,k1):\n",
    "                # Check on uncertainty. If too high, remove triplet\n",
    "                if limit_taken_into_accout:\n",
    "                    eig1 = np.linalg.eigvals(i2[2])**0.5\n",
    "                    eig2 = np.linalg.eigvals(j2[2])**0.5\n",
    "                    eig3 = np.linalg.eigvals(k2[2])**0.5\n",
    "                    if eig1[0]<limit_uncertainty and eig1[1]<limit_uncertainty and eig1[2]<limit_uncertainty \\\n",
    "                            and eig2[0]<limit_uncertainty and eig2[1]<limit_uncertainty and eig2[2]<limit_uncertainty \\\n",
    "                            and eig3[0]<limit_uncertainty and eig3[1]<limit_uncertainty and eig3[2]<limit_uncertainty:\n",
    "                        MC_1_inter.append(i2)\n",
    "                        MC_2_inter.append(j2)\n",
    "                        MC_3_inter.append(k2)\n",
    "                else:\n",
    "                    MC_1_inter.append(i2)\n",
    "                    MC_2_inter.append(j2)\n",
    "                    MC_3_inter.append(k2)\n",
    "\n",
    "        if save_MC_inteprolated:\n",
    "            if Sensor!=\"Fake\":\n",
    "                theodo_u.save_MC_interpolated_sorted(MC_1_inter, file_i + \"uncertainty/interpolation/MC_\"+str(Sensor)+\"_\"+str(model_chosen[0])+\"_\"+str(model_chosen[1])+\"_\"+str(model_chosen[2])+\"_\"+str(model_chosen[3])+\"_\"+str(model_chosen[4])+\"_1.csv\")\n",
    "                theodo_u.save_MC_interpolated_sorted(MC_2_inter, file_i + \"uncertainty/interpolation/MC_\"+str(Sensor)+\"_\"+str(model_chosen[0])+\"_\"+str(model_chosen[1])+\"_\"+str(model_chosen[2])+\"_\"+str(model_chosen[3])+\"_\"+str(model_chosen[4])+\"_2.csv\")\n",
    "                theodo_u.save_MC_interpolated_sorted(MC_3_inter, file_i + \"uncertainty/interpolation/MC_\"+str(Sensor)+\"_\"+str(model_chosen[0])+\"_\"+str(model_chosen[1])+\"_\"+str(model_chosen[2])+\"_\"+str(model_chosen[3])+\"_\"+str(model_chosen[4])+\"_3.csv\")\n",
    "            else:\n",
    "                theodo_u.save_MC_interpolated_sorted(MC_1_inter, file_i + \"uncertainty/interpolation/MC_\"+str(Sensor)+\"_\"+str(model_chosen[0])+\"_\"+str(model_chosen[1])+\"_\"+str(model_chosen[2])+\"_\"+str(model_chosen[3])+\"_\"+str(model_chosen[4])+\"_\"+str(rate_fake)+\"_1.csv\")\n",
    "                theodo_u.save_MC_interpolated_sorted(MC_2_inter, file_i + \"uncertainty/interpolation/MC_\"+str(Sensor)+\"_\"+str(model_chosen[0])+\"_\"+str(model_chosen[1])+\"_\"+str(model_chosen[2])+\"_\"+str(model_chosen[3])+\"_\"+str(model_chosen[4])+\"_\"+str(rate_fake)+\"_2.csv\")\n",
    "                theodo_u.save_MC_interpolated_sorted(MC_3_inter, file_i + \"uncertainty/interpolation/MC_\"+str(Sensor)+\"_\"+str(model_chosen[0])+\"_\"+str(model_chosen[1])+\"_\"+str(model_chosen[2])+\"_\"+str(model_chosen[3])+\"_\"+str(model_chosen[4])+\"_\"+str(rate_fake)+\"_3.csv\")\n",
    "\n",
    "        MC_1_sorted = []\n",
    "        MC_2_sorted = []\n",
    "        MC_3_sorted = []\n",
    "        if limit_taken_into_accout:\n",
    "            for i1,j1,k1 in zip(MC_1_inter,MC_2_inter,MC_3_inter):\n",
    "                d1 = abs(np.linalg.norm(i1[1]-j1[1])-extrinsic_calibration_results[0])\n",
    "                d2 = abs(np.linalg.norm(i1[1]-k1[1])-extrinsic_calibration_results[1])\n",
    "                d3 = abs(np.linalg.norm(k1[1]-j1[1])-extrinsic_calibration_results[2])\n",
    "                if d1<limit_dist and d2<limit_dist and d3<limit_dist:\n",
    "                    MC_1_sorted.append(i1)\n",
    "                    MC_2_sorted.append(j1)\n",
    "                    MC_3_sorted.append(k1)\n",
    "        else:\n",
    "            MC_1_sorted = MC_1_inter\n",
    "            MC_2_sorted = MC_2_inter\n",
    "            MC_3_sorted = MC_3_inter\n",
    "\n",
    "        if want_sensor_pose:\n",
    "            if Sensor!=\"Fake\":\n",
    "                P_sensor = theodo_g.chose_sensor_before_ptp(path, Sensor, Gps_reference_chosen)\n",
    "                Pose_sensor_MC = []\n",
    "                for i_mc,j_mc,k_mc in zip(MC_1_sorted,MC_2_sorted,MC_3_sorted):\n",
    "                    Pose_sensor = []\n",
    "                    p1_corrected = theodo_g.return_point_from_covariance(i_mc[2],i_mc[1], num_samples_MC_sensor)\n",
    "                    p2_corrected = theodo_g.return_point_from_covariance(j_mc[2],j_mc[1], num_samples_MC_sensor)\n",
    "                    p3_corrected = theodo_g.return_point_from_covariance(k_mc[2],k_mc[1], num_samples_MC_sensor)\n",
    "                    for i,j,k in zip(p1_corrected,p2_corrected,p3_corrected):\n",
    "                        Q = np.array([i, j, k]).T\n",
    "                        Q =np.concatenate((Q, np.array([[1,1,1]])), axis=0)\n",
    "                        T = theodo_u.point_to_point_minimization(P_sensor, Q)\n",
    "                        Pose_sensor.append(T)\n",
    "                    p_T, mu_T, cov_T = theodo_g.find_noise_list_tf(Pose_sensor)\n",
    "                    Pose_sensor_MC.append([i_mc[0],mu_T, cov_T])\n",
    "            else:\n",
    "                P_sensor = theodo_g.chose_sensor_before_ptp(path, Sensor_fake, Gps_reference_chosen)\n",
    "                Pose_sensor_MC = []\n",
    "                for i_mc,j_mc,k_mc in zip(MC_1_sorted,MC_2_sorted,MC_3_sorted):\n",
    "                    Pose_sensor = []\n",
    "                    p1_corrected = theodo_g.return_point_from_covariance(i_mc[2],i_mc[1], num_samples_MC_sensor)\n",
    "                    p2_corrected = theodo_g.return_point_from_covariance(j_mc[2],j_mc[1], num_samples_MC_sensor)\n",
    "                    p3_corrected = theodo_g.return_point_from_covariance(k_mc[2],k_mc[1], num_samples_MC_sensor)\n",
    "                    for i,j,k in zip(p1_corrected,p2_corrected,p3_corrected):\n",
    "                        Q = np.array([i, j, k]).T\n",
    "                        Q =np.concatenate((Q, np.array([[1,1,1]])), axis=0)\n",
    "                        T = theodo_u.point_to_point_minimization(P_sensor, Q)\n",
    "                        Pose_sensor.append(T)\n",
    "                    p_T, mu_T, cov_T = theodo_g.find_noise_list_tf(Pose_sensor)\n",
    "                    Pose_sensor_MC.append([i_mc[0],mu_T, cov_T])\n",
    "\n",
    "            if save_MC_sensor:\n",
    "                if Sensor==\"GNSS\":\n",
    "                    theodo_u.save_MC_interpolated_sorted(Pose_sensor_MC, file_i + \"uncertainty/sensor/\"+Sensor+\"_\"+str(Gps_reference_chosen)+\"_\"\n",
    "                                                         +str(model_chosen[0])+\"_\"+str(model_chosen[1])+\"_\"+str(model_chosen[2])\n",
    "                                                     +\"_\"+str(model_chosen[3])+\"_\"+str(model_chosen[4])+\".csv\")\n",
    "                else:\n",
    "                    theodo_u.save_MC_interpolated_sorted(Pose_sensor_MC, file_i + \"uncertainty/sensor/\"+Sensor+\"_\"\n",
    "                                                         +str(model_chosen[0])+\"_\"+str(model_chosen[1])+\"_\"+str(model_chosen[2])\n",
    "                                                     +\"_\"+str(model_chosen[3])+\"_\"+str(model_chosen[4])+\".csv\")\n",
    "\n",
    "        \n",
    "        print(\"Interpolation finished !\")\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a76824919608da607e2a40354fe3c77d7bc1244b46ebc9b3c31b8cd096d36c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
